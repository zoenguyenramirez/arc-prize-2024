{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69216b2d-1534-471d-b669-e16994d2cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from plot_funcs import plot_task\n",
    "from   matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# %matplotlib widget\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59029566-e018-45ca-93eb-64bc3fac1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from src.model import Transformer\n",
    "from src.token import VOCAB_SIZE, SpecialToken\n",
    "from src.load_data import load_from_json, GridDataset\n",
    "from src.utils.helper import set_deterministic\n",
    "from src.utils.display_diff import compare_sequences, colorize, split_into_chunks\n",
    "from src.utils.transformer_helper import create_mask\n",
    "\n",
    "from checkpoint_plot import format_batch, plot_task, plot_answer\n",
    "\n",
    "debug_on_cpu = False\n",
    "\n",
    "def generate_sample(model, input_sequence, max_length, device, index_to_visualize: List[int], override_this_index: Optional[List[int]] = None, *, mask_hack):\n",
    "    assert override_this_index is None, override_this_index\n",
    "    model.eval()\n",
    "    y = 0\n",
    "    x = 0\n",
    "    coord = (-1, -1)\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(input_sequence['task'], dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_length)\n",
    "\n",
    "        seq_length = len(input_sequence['task'])\n",
    "        print('seq_length', seq_length, max_length)\n",
    "        for generated_token_index in range(max_length - seq_length):\n",
    "            activation_store.clear()\n",
    "\n",
    "            mask = create_mask(input_ids, device, [seq_length], mask_hack=mask_hack)\n",
    "            outputs = model(input_ids, mask)  # (1, seq_length, vocab_size)\n",
    "            next_token_logits = outputs[0, -1, :]  # (vocab_size)\n",
    "\n",
    "            if generated_token_index == index_to_visualize[0]:\n",
    "                # visualize_activation(model, input_ids, next_token_logits)\n",
    "                torch.save({\n",
    "                        'activations': activation_store.activations,\n",
    "                        'input_ids': input_ids,\n",
    "                        'outputs': outputs\n",
    "                    },\n",
    "                    f'../temp/{generated_token_index}_activation.pt')    \n",
    "\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "            print(f'next_token_id [{seq_length + generated_token_index}]: {next_token_id}')\n",
    "\n",
    "            try:\n",
    "                the_index = index_to_visualize.index(generated_token_index)\n",
    "                if override_this_index is not None:\n",
    "                    next_token_id = override_this_index[the_index]\n",
    "                    print('override_this_index', generated_token_index, next_token_id)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "\n",
    "            if next_token_id < SpecialToken.CELL_TOKEN_SIZE.value:\n",
    "                coord = (y, x)\n",
    "                x = x + 1\n",
    "                x = min(x, model.max_grid_size - 1)\n",
    "            elif next_token_id == SpecialToken.ROW_SEPARATOR.value:\n",
    "                coord = (y, x)\n",
    "                x = 0\n",
    "                y = y + 1\n",
    "                y = min(y, model.max_grid_size - 1)\n",
    "            else:\n",
    "                y = 0\n",
    "                x = 0\n",
    "                coord = (-1, -1)\n",
    "\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([[[next_token_id, coord[0], coord[1], -1, -1]]], dtype=torch.long, device=device)], dim=1)  # (1, seq_length + 1)\n",
    "            if next_token_id == SpecialToken.END.value:\n",
    "                break\n",
    "            \n",
    "        return input_ids.squeeze(0).tolist()  # (seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361d766-4d83-4934-a5b7-d7bfe3c4463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze_tensor import analyze_tensor\n",
    "\n",
    "\n",
    "def show_interesting_attention(attention_weights, input_ids, seq_length, grid_index, top_k=5):        \n",
    "    # Sum attention weights across rows to get column importance\n",
    "    column_importance = attention_weights.sum(dim=0) # (seq_len)\n",
    "    assert column_importance.dim() == 1\n",
    "    assert column_importance.shape == (seq_length,), f\"got {column_importance.shape}, not ({seq_length})\"\n",
    "\n",
    "    # Get the indices of the top-k interesting columns\n",
    "    top_k_values, top_k_indices = torch.topk(column_importance, k=top_k)\n",
    "    assert top_k_values.shape == top_k_indices.shape == (top_k,), f\"got {top_k_values.shape}\"\n",
    "\n",
    "    print('grid_index', grid_index)\n",
    "    print(f\"Top {top_k} interesting attention columns for the current layer:\")\n",
    "       \n",
    "    for i, (index, value) in enumerate(zip(top_k_indices.tolist(), top_k_values.tolist()), 1):\n",
    "        token = input_ids[0, index]\n",
    "        # Find the corresponding grid index\n",
    "        grid_position = torch.searchsorted(grid_index, index, right=True).item()\n",
    "        \n",
    "        print(f\"{i}. Index: {index}, grid:{grid_position}, Token: '{token}', Attention Sum: {value}\")\n",
    "\n",
    "def visualize_activation(model, input_ids, outputs):\n",
    "    # Find all SpecialToken.START_INPUT.value and SpecialToken.START_OUTPUT.value in input_ids[0, :, 0]\n",
    "    grid_index = torch.where((input_ids[0, :, 0] == SpecialToken.START_INPUT.value) | \n",
    "                             (input_ids[0, :, 0] == SpecialToken.START_OUTPUT.value))[0]\n",
    "\n",
    "    print('grid_index', grid_index)\n",
    "\n",
    "    transformer_input_x = activation_store['transformer_input_x']\n",
    "    assert len(transformer_input_x) == 1\n",
    "    transformer_input_x = transformer_input_x[0]\n",
    "    torch.set_printoptions(profile=\"full\")\n",
    "    # print('transformer_input_x', transformer_input_x)\n",
    "    analyze_tensor(transformer_input_x, f\"transformer_input_x ({transformer_input_x.shape})\")\\\n",
    "\n",
    "    # transformer_input_mask = activation_store['transformer_input_mask']\n",
    "    # assert len(transformer_input_mask) == 1\n",
    "    # transformer_input_mask = transformer_input_mask[0]\n",
    "    # analyze_tensor(transformer_input_mask, f\"transformer_input_mask ({transformer_input_mask.shape})\")\n",
    "\n",
    "    embedding_out = activation_store['embedding_out']\n",
    "    assert len(embedding_out) == 1\n",
    "    embedding_out = embedding_out[0]\n",
    "    analyze_tensor(embedding_out, f\"embedding_out ({embedding_out.shape})\")\n",
    "\n",
    "    initial_dropout_out = activation_store['initial_dropout_out']\n",
    "    assert len(initial_dropout_out) == 1\n",
    "    initial_dropout_out = initial_dropout_out[0]\n",
    "    analyze_tensor(initial_dropout_out, f\"initial_dropout_out ({initial_dropout_out.shape})\")\n",
    "    \n",
    "    num_layers = len(model.layers)\n",
    "    assert num_layers == len(activation_store['attention_weights'])\n",
    "    assert num_layers == len(activation_store['attention_out'])\n",
    "    \n",
    "    for layer_index in range(num_layers): # range(2): # \n",
    "        attention_weights = activation_store['attention_weights'][layer_index]\n",
    "        \n",
    "        batch, head, seq_length, _ = attention_weights.shape\n",
    "\n",
    "        for head_index in range(head):\n",
    "            attention_weights_slice = attention_weights[0, head_index, :, :]\n",
    "            attention_weights_slice = attention_weights_slice.squeeze()\n",
    "            \n",
    "            assert attention_weights_slice.dim() == 2, f\"Got {attention_weights_slice.shape}\"\n",
    "\n",
    "            analyze_tensor(attention_weights_slice, f\"[{layer_index}].attention_weights[{head_index}] ({attention_weights_slice.shape})\")\n",
    "            show_interesting_attention(attention_weights_slice, input_ids, seq_length, grid_index, 45)\n",
    "\n",
    "        attention_out = activation_store['attention_out'][layer_index]\n",
    "        analyze_tensor(attention_out, f\"[{layer_index}].attention_out ({attention_out.shape})\")\n",
    "        norm1_out = activation_store['norm1_out'][layer_index]\n",
    "        analyze_tensor(norm1_out, f\"[{layer_index}].norm1_out ({norm1_out.shape})\")\n",
    "        norm2_out = activation_store['norm2_out'][layer_index]\n",
    "        analyze_tensor(norm2_out, f\"[{layer_index}].norm2_out ({norm2_out.shape})\")\n",
    "        \n",
    "    analyze_tensor(outputs, f\"output ({outputs.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702274a6-16c0-442c-a565-03ac6384d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.activation_hooks import ActivationStore, register_transformer_hooks\n",
    "from src.checkpoint_handler import CheckpointHandler\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "# Load data\n",
    "data_sources = ['arc-agi_evaluation'] # ['synth_conditional_logic_test'] \n",
    "all_challenges = {}\n",
    "\n",
    "for source in data_sources:\n",
    "    try:\n",
    "        challenges, solutions = load_from_json(source, '../input_data/')\n",
    "        all_challenges.update(challenges)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading {source}: {e}. Skipping this data source.\")\n",
    "\n",
    "if not all_challenges:\n",
    "    print(\"No data could be loaded. Please check the file paths and data sources.\")\n",
    "    \n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and not debug_on_cpu) else \"cpu\")\n",
    "checkpoint_path='../cloud_runs/69.55.141.119/barc/runs/barc/20241107_224744_nogit_nobranch_lr4e-05_bl1e-06_ssu0_bs16_h4_es888_nl18_we10_as1_ph1_ac1_ad1_scosine_oadam_ge1_mh0_ssnone_ss1e-02_c6/Transformer_best_7738.pt'\n",
    "model, max_seq_length, checkpoint_args = CheckpointHandler.load_checkpoint_in_production(checkpoint_path, device, adjust_max_length=12000)\n",
    "mask_hack = checkpoint_args.get('mask_hack', True)\n",
    "\n",
    "activation_store = ActivationStore()\n",
    "register_transformer_hooks(model, activation_store)\n",
    "\n",
    "model.activate_attention_weights_output(True)\n",
    "model.eval()\n",
    "\n",
    "# Create the dataset\n",
    "dataset_ref = GridDataset.load_from_paired_file(all_challenges, solutions, second_only=False)\n",
    "dataset = GridDataset.load_from_paired_file(all_challenges, None, second_only=False)\n",
    "\n",
    "print('dataset', len(dataset), type(dataset[0]) if len(dataset) > 0 else '?')\n",
    "print('dataset_ref', len(dataset_ref), type(dataset_ref[0]))\n",
    "print('max_seq_length', max_seq_length)\n",
    "\n",
    "mismatch_count = 0    \n",
    "tested_count = 0\n",
    "# Generate samples\n",
    "start_index = 109\n",
    "for i in range(start_index, len(dataset)):\n",
    "    print('index', i)\n",
    "    input_sequence = dataset[i]\n",
    "\n",
    "    if max_seq_length > len(input_sequence['task']):\n",
    "        tested_count += 1\n",
    "        \n",
    "        sample = generate_sample(model, input_sequence, max_seq_length, device, [1], mask_hack=mask_hack) # [2, 5, 7, 8], override_this_index=[8, 8, 0, 8])\n",
    "        # plot_answer(input_sequence['task'], sample, i)\n",
    "\n",
    "        expected_sequence = dataset_ref[i]\n",
    "        # plot_answer(expected_sequence['task'], expected_sequence['task'], i)\n",
    "    else:\n",
    "        print('too long to evaluate')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6c85e-77a9-4ec8-95ea-228f1b82be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../temp/1_activation.pt', map_location=torch.device('cpu'))\n",
    "attention_weights = checkpoint['activations']['attention_weights']\n",
    "input_ids = checkpoint['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071962f6-963d-4c49-a4d2-8389703086bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import copy\n",
    "from matplotlib.backend_bases import MouseButton\n",
    "\n",
    "from src.token import SpecialToken\n",
    "from src.utils.helper import detokenize_grid\n",
    "\n",
    "# Create output widget\n",
    "out = widgets.Output()\n",
    "display(out)\n",
    "\n",
    "cmap = colors.ListedColormap(\n",
    "    ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
    "     '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "norm = colors.Normalize(vmin=0, vmax=9)\n",
    "\n",
    "def to_grid_index(i, train_or_test, input_or_output):\n",
    "    return i * 2 + (1 if input_or_output == 'output' else 0) + 1\n",
    "\n",
    "def find_cell_index(input_sequence, y, x, grid_index):\n",
    "    cell_index = next(index for index, element in enumerate(input_sequence) if (lambda cell: cell[1]==y and cell[2] == x and cell[4] == grid_index)(element))\n",
    "    return cell_index\n",
    "\n",
    "def find_grid_width(input_sequence_segment):\n",
    "    cell_index = next(index for index, element in enumerate(input_sequence_segment) if (lambda cell: cell[0]==SpecialToken.ROW_SEPARATOR.value)(element))\n",
    "    return cell_index\n",
    "    \n",
    "def apply_attention_overlay(y, x, i, train_or_test, input_or_output, context, all_axes):\n",
    "    grid_index = to_grid_index(i, train_or_test, input_or_output)\n",
    "    # print('grid_index', grid_index, y, x, train_or_test, input_or_output)\n",
    "\n",
    "    cell_index = find_cell_index(context['input_sequence'], y, x, grid_index)\n",
    "    # print(cell_index)     # , context['input_sequence']\n",
    "\n",
    "    attention_vector = context['attention_head'][cell_index, :]\n",
    "    min_att = attention_vector.min().item()\n",
    "    assert min_att >= 0\n",
    "    max_att = attention_vector.max().item()\n",
    "    \n",
    "    for ax in all_axes:\n",
    "        for img in ax.get_images():\n",
    "            if hasattr(img, 'original_data'):\n",
    "                i, train_or_test, input_or_output = img.original_data[:3]\n",
    "                target_grid_index = to_grid_index(i, train_or_test, input_or_output)\n",
    "                target_grid_cell_start_index = find_cell_index(context['input_sequence'], 0, 0, target_grid_index)\n",
    "                # width = find_grid_width(context['input_sequence'][target_grid_ceel_start_index:])\n",
    "                input_matrix = img.original_data[3]\n",
    "                height = len(input_matrix)\n",
    "                width = len(input_matrix[0])\n",
    "\n",
    "                attention_seq = attention_vector[target_grid_cell_start_index:]\n",
    "\n",
    "                # print(i, train_or_test, input_or_output, target_grid_index, height, width)\n",
    "                  \n",
    "                # Reshape attention weights to match grid shape\n",
    "                attention_grid = np.zeros((height, width))\n",
    "                for h in range(height):\n",
    "                    for w in range(width):\n",
    "                        attention_grid[h][w] = attention_seq[h * (width + 1) + w]\n",
    "\n",
    "                # Normalize attention weights to [0,1]\n",
    "                attention_grid = attention_grid / max_att\n",
    "                \n",
    "                # Remove existing attention overlay if any\n",
    "                for artist in ax.get_children():\n",
    "                    if getattr(artist, 'is_attention', False): # isinstance(artist, plt.matplotlib.collections.QuadMesh) and \n",
    "                        # print('remove old one')\n",
    "                        artist.remove()\n",
    "                \n",
    "                # Add new attention overlay\n",
    "                attention_overlay = ax.imshow(attention_grid, \n",
    "                                           cmap='Reds', \n",
    "                                           alpha=0.999,\n",
    "                                           interpolation='nearest')\n",
    "                attention_overlay.is_attention = True                \n",
    "\n",
    "def plot_one(ax, task_data, i, train_or_test, input_or_output, context):\n",
    "    try:\n",
    "        input_matrix = task_data[train_or_test][i][input_or_output]\n",
    "    except:\n",
    "        return\n",
    "        \n",
    "    im = ax.imshow(input_matrix, cmap=cmap, norm=norm)\n",
    "    ax.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "    \n",
    "    plt.setp(plt.gcf().get_axes(), xticklabels=[], yticklabels=[])\n",
    "    ax.set_xticks([x-0.5 for x in range(1 + len(input_matrix[0]))])     \n",
    "    ax.set_yticks([x-0.5 for x in range(1 + len(input_matrix))])\n",
    "    \n",
    "    # Calculate font size based on grid dimensions\n",
    "    grid_size = max(len(input_matrix), len(input_matrix[0]))\n",
    "    base_font_size = 10  # Base font size for small grids\n",
    "    min_font_size = 4    # Minimum font size to ensure readability\n",
    "    font_size = max(base_font_size - (grid_size - 5) * 0.5, min_font_size)\n",
    "    \n",
    "    # Add text annotations with adjusted font size\n",
    "    for y in range(len(input_matrix)):\n",
    "        for x in range(len(input_matrix[0])):\n",
    "            value = input_matrix[y][x]\n",
    "            text_color = 'white' if value > 5 or value == 0 else 'black'\n",
    "            ax.text(x, y, str(value), ha='center', va='center', color=text_color, fontsize=font_size)\n",
    "    \n",
    "    ax.set_title(f'{train_or_test} {input_or_output}', color='black' if train_or_test == 'train' else 'red')\n",
    "    im.original_data = (i, train_or_test, input_or_output, input_matrix)\n",
    "\n",
    "    # Add hover functionality\n",
    "    # Define click handler with output capture\n",
    "    @out.capture()\n",
    "    def onclick(event):\n",
    "        if event.inaxes == ax:\n",
    "            if event.button is MouseButton.LEFT:\n",
    "                x, y = int(round(event.xdata)), int(round(event.ydata))\n",
    "                # print('onclick!', x, y)\n",
    "                if 0 <= x < len(input_matrix[0]) and 0 <= y < len(input_matrix):\n",
    "                    all_axes = fig.get_axes()\n",
    "                    for img in ax.get_images():\n",
    "                        if hasattr(img, 'original_data'):\n",
    "                            apply_attention_overlay(y, x, *img.original_data[:3], context, all_axes)\n",
    "                            break\n",
    "                    fig.canvas.draw_idle()           \n",
    "                    \n",
    "            else:\n",
    "                for i_ax in fig.get_axes():\n",
    "                    for artist in i_ax.get_children():\n",
    "                        if getattr(artist, 'is_attention', False): # isinstance(artist, plt.matplotlib.collections.QuadMesh) and \n",
    "                            # print('remove old one')\n",
    "                            artist.remove()                        \n",
    "                fig.canvas.draw_idle()\n",
    "\n",
    "    fig = ax.figure\n",
    "    result = fig.canvas.mpl_connect(\"button_press_event\", onclick)\n",
    "\n",
    "def plot_task(task, i, t, context):\n",
    "    \"\"\"    Plots the first train and test pairs of a specified task,\n",
    "    using same color scheme as the ARC app    \"\"\"    \n",
    "\n",
    "    num_train = len(task['train'])\n",
    "    num_test  = len(task['test'])\n",
    "\n",
    "    w = num_train + num_test\n",
    "    fig, axs = plt.subplots(2, w, figsize=(3*w, 3*2))\n",
    "    plt.suptitle(f'Set #{i}, {t}:', fontsize=20, fontweight='bold', y=1)\n",
    "\n",
    "    for j in range(num_train):     \n",
    "        plot_one(axs[0, j], task, j, 'train', 'input', context)\n",
    "        plot_one(axs[1, j], task, j, 'train', 'output', context)        \n",
    "    for j in range(num_test):     \n",
    "        plot_one(axs[0, num_train + j], task, j, 'test', 'input', context)\n",
    "        plot_one(axs[1, num_train + j], task, j, 'test', 'output', context)        \n",
    "       \n",
    "    fig.patch.set_linewidth(5)\n",
    "    fig.patch.set_edgecolor('black') \n",
    "    fig.patch.set_facecolor('#dddddd')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()  \n",
    "\n",
    "def plot_interactive_activation(input_sequence, attention_head, attention_index):\n",
    "    task = {'train': [], 'test': []}\n",
    "\n",
    "    context = {\n",
    "        'attention_head': attention_head,\n",
    "        'input_sequence': input_sequence\n",
    "    }\n",
    "    \n",
    "    current_section = None\n",
    "    current_data = []\n",
    "    input_length = len(input_sequence)\n",
    "    \n",
    "    for index, element in enumerate(input_sequence):\n",
    "        token = element[0]\n",
    "        if token == SpecialToken.START_INPUT.value:\n",
    "            current_section = 'input'\n",
    "            current_data = []\n",
    "        elif token == SpecialToken.END_INPUT.value:\n",
    "            if current_section == 'input':\n",
    "                task['train'].append({'input': detokenize_grid(current_data)})\n",
    "        elif token == SpecialToken.START_OUTPUT.value:\n",
    "            current_section = 'output'\n",
    "            current_data = []\n",
    "        elif token == SpecialToken.END_OUTPUT.value:\n",
    "            if current_section == 'output':\n",
    "                output_data = {'output': detokenize_grid(current_data)}\n",
    "                if index < input_length:\n",
    "                    task['train'][-1].update(output_data)\n",
    "                else:\n",
    "                    print ('new answer', output_data)\n",
    "                    last_train = task['train'].pop()\n",
    "                    last_train.update(output_data)\n",
    "                    task['test'].append(last_train)                    \n",
    "        else:\n",
    "            if token < 10 or token == SpecialToken.ROW_SEPARATOR.value:\n",
    "                current_data.append(token)\n",
    "                \n",
    "    # Plot task\n",
    "    plot_task(task, 0, f\"attention index {attention_index}\", context)\n",
    "\n",
    "def main():\n",
    "    # print('fetched', sequence)\n",
    "    # print('detokenized', task)    \n",
    "    for attention_index, attention in enumerate(attention_weights):\n",
    "    \n",
    "        # if attention_index <= -1:\n",
    "        #     continue\n",
    "        \n",
    "        assert attention.shape[0] == 1 # batch is 1\n",
    "        \n",
    "        for attention_head in range(attention.shape[1]):\n",
    "            attention_head = attention[0, attention_head]\n",
    "            assert input_ids.shape[1] == attention_head.shape[0]\n",
    "    \n",
    "            plot_interactive_activation(input_ids[0].tolist(), attention_head, attention_index)\n",
    "            # break\n",
    "        # break\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3d261-91e2-40f0-8695-9732965ea9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
